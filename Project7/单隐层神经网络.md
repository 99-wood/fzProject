# 单隐层神经网络实验报告

## 题目要求

1. 使用 C 语言（或伪代码）编写实现“单隐层神经网络”及其训练过程。
2. 自己构建大于等于 $100$ 个数据的训练数据集，并进行训练。
3. 给出训练后神经网络的性能评价值。
4. 讨论更改学习率 $\eta$ 的值、循环次数对训练的影响。

## 实现思路

1. 搭建单隐藏层的前馈网络（Multilayer Feedforward Network）。一个输入层（Input Layer），一个隐藏层（Hidden Layer），一个输出层（Output Layer）。
2. 训练采用监督式学习，提供多组样例输入与输出。
3. 权重的更新采用误差反向传播算法（Backpropagation，简称 BP 算法）。

## 实现细节

### 变量定义

构建 InputLayer、HiddenLayer、OutputLayer 结构体。其中包含权值 w 数组和输出 output 数组。

其中 $w_{i,j}$ 表示当前层第 $i$ 个节点与上一层第 $j$ 个节点之间的权值。$output_i$ 表示当前层第 $i$ 个节点的输出。

全局变量 eta 为学习率。

### 初始化

通过预设或输入的输入层，隐藏层，输出层的大小构建神经网络。初始权值使用 $0$ 到 $1$ 之间的随机数。

```cpp
void init(){
    for(int i = 1; i <= hiddenLayer.siz; ++i){
        hiddenLayer.w[i][0] = (double)rand() / RAND_MAX * inputLayer.siz;
        for(int j = 1; j <= inputLayer.siz; ++j){
            hiddenLayer.w[i][j] = (double)rand() / RAND_MAX;
        }
    }
    for(int i = 1; i <= outputLayer.siz; ++i){
    	outputLayer.w[i][0] = (double)rand() / RAND_MAX * hiddenLayer.siz;
        for(int j = 1; j <= hiddenLayer.siz; ++j){
            outputLayer.w[i][j] = (double)rand() / RAND_MAX;
        }
    }
    return;
}
```

### 运行

依次计算节点的输出值并向后传递。

```cpp
void run(){
    inputLayer.output[0] = -1;
    for(int i = 1; i <= inputLayer.siz; ++i){
        inputLayer.output[i] = input[i];
    }
    hiddenLayer.output[0] = -1;
    for(int i = 1; i <= hiddenLayer.siz; ++i){
        double ans = 0;
        for(int j = 0; j <= inputLayer.siz; ++j){
            ans += inputLayer.output[j] * hiddenLayer.w[i][j];
        }
        ans = f(ans);
        hiddenLayer.output[i] = ans;
    }
    for(int i = 1; i <= outputLayer.siz; ++i){
        double ans = 0;
        for(int j = 0; j <= hiddenLayer.siz; ++j){
            ans += hiddenLayer.output[j] * outputLayer.w[i][j];
        }
        ans = f(ans);
        outputLayer.output[i] = ans;
    }
    return;
}
```

### 更新

```cpp
void update(){
    for(int i = 1; i <= outputLayer.siz; ++i){
        g[i] = outputLayer.output[i] * (1.0 - outputLayer.output[i]) * (output[i] - outputLayer.output[i]);
    }
    for(int i = 1; i <= hiddenLayer.siz; ++i){
        double sum = 0;
        for(int j = 1; j <= outputLayer.siz; ++j){
            sum += outputLayer.w[j][i] * g[j];
        }
        e[i] = hiddenLayer.output[i] * (1.0 - hiddenLayer.output[i]) * sum;
    }
    for(int i = 1; i <= outputLayer.siz; ++i){
        for(int j = 0; j <= hiddenLayer.siz; ++j){
            outputLayer.w[i][j] += eta * g[i] * hiddenLayer.output[j];
        }
    }
    for(int i = 1; i <= hiddenLayer.siz; ++i){
        for(int j = 0; j <= inputLayer.siz; ++j){
            hiddenLayer.w[i][j] += eta * e[i] * inputLayer.output[j];
        }
    }
    return;
}
```

